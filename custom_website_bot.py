# -*- coding: utf-8 -*-
"""Custom_Website_Bot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Hk1xACat4N3HHhZ9Jx7EQgX9mypwIhOY

# 1. Install All Required Packages
"""

!pip -q install langchain langchain_community
!pip -q install pypdf
!pip -q install sentence_transformers
!pip -q install datasets loralib sentencepiece
!pip -q install bitsandbytes accelerate transformers

!pip -q install unstructured
!pip install pinecone

!pip install tokenizers

!pip install xformers

"""# 2. Import all the Required Libraries"""

import os
from langchain.document_loaders import UnstructuredURLLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQAWithSourcesChain
from langchain.embeddings import HuggingFaceEmbeddings
from langchain import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import pipeline
from huggingface_hub import notebook_login
import textwrap
import sys
import torch

import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

"""# 3. Pass the URLs and extract the data from these URLs"""

URLs = [
    'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb',
    'https://www.databricks.com/blog/mpt-7b',
    'https://lmsys.org/blog/2023-03-30-vicuna/',
    'https://stability.ai/news/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models'
]

loaders = UnstructuredURLLoader(urls=URLs)
data = loaders.load()

data

len(data)

"""# 4. Split the Text into Chunks

"""

text_splitter = CharacterTextSplitter(separator = '\n',
                                      chunk_size=1000,
                                      chunk_overlap=200)
docs = text_splitter.split_documents(data)

len(docs)

docs

docs[0]

docs[1]

"""# 5. Download the Hugging Face Embeddings"""

embeddings = HuggingFaceEmbeddings()

embeddings

query_result = embeddings.embed_query("Hello World")
len(query_result)

query_result

"""# 6. Convert the Text Chunks into Embeddings and Create a Knowledge Base

"""

import pinecone
import os

os.environ["PINECONE_API_KEY"] = "pcsk_4MqwfK_Ew2McJ1VDDHvhZT22gqPHN4xUYYnGiso7M8HS9qPxxncQ1dTPNR2aw23PRr6nS8"
os.environ["PINECONE_ENVIRONMENT"] = "us-east-1"

from pinecone import Pinecone

pc = Pinecone(
      api_key=os.environ.get("PINECONE_API_KEY")
)

index_name = "webbot"
index = pc.Index(index_name)

vectors = []
for i, doc in enumerate(docs):
    vector = embeddings.embed_query(doc.page_content)  # returns list of floats
    metadata = {
        "text": doc.page_content
    }
    vectors.append((f"doc{i+1}", vector, metadata))

vectors[0]

index.upsert(vectors=vectors)

"""# 7. Create a Large Language Model Wrapper"""

notebook_login()

model = "meta-llama/Llama-2-7b-chat-hf"

tokenizer = AutoTokenizer.from_pretrained(model, use_auth_token=True,)

model = AutoModelForCausalLM.from_pretrained(model,
                                             device_map='auto',
                                             torch_dtype=torch.float16,
                                             use_auth_token=True,
                                             load_in_8bit=True,)

pipe = pipeline("text-generation",
                model=model,
                tokenizer=tokenizer,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                max_new_tokens=512,
                do_sample=True,
                top_k=30,
                num_return_sequences=1,
                eos_token_id=tokenizer.eos_token_id)

llm = HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0})

llm.invoke("Please provide a concise summary of the Book Harry Potter")

"""# Retrieve documents for Vector DB and Test

"""

def retrieve_documents(query: str, top_k: int = 3):
    """Embed query, search Pinecone, and return text content"""
    query_vector = embeddings.embed_query(query)
    results = index.query(vector=query_vector, top_k=top_k, include_metadata=True)

    docs = []
    for match in results["matches"]:
        docs.append({
            "text": match["metadata"].get("text", ""),
            "score": match["score"]
        })
    return docs

def retrieval_qa(question: str, top_k: int = 3):
    # Step 1: Retrieve top documents
    retrieved_docs = retrieve_documents(question, top_k)

    # Step 2: Combine retrieved text into a single context
    context = "\n\n".join([doc["text"] for doc in retrieved_docs])

    # Step 3: Construct the prompt (like chain_type="stuff")
    prompt = f"""
You are a helpful assistant. Use the context below to answer the question.
If the answer isn't clear, say so. Be concise and accurate.

Context:
{context}

Question: {question}

Answer:
"""

    # Step 4: Query the LLM
    response = llm.invoke(prompt)

    # Step 5: Handle both possible return types (string or object)
    if isinstance(response, str):
        answer_text = response.strip()
    else:
        # if it's a ChatMessage / LLMResult-like object
        answer_text = getattr(response, "content", str(response)).strip()

    # Step 6: Return result
    return {
        "answer": answer_text,
        "retrieved_texts": [doc["text"][:200] + "..." for doc in retrieved_docs],
        "scores": [doc["score"] for doc in retrieved_docs]
    }

query = "How does Llama 2 outperform other models?"
result = retrieval_qa(query)

print("Answer:\n", result["answer"])
print("\nTop Retrieved Chunks:")
for i, snippet in enumerate(result["retrieved_texts"], 1):
    print(f"{i}. ({result['scores'][i-1]:.3f}) {snippet}")

query = "How good is Vicuna?"
result = retrieval_qa(query)

print("Answer:\n", result["answer"])
print("\nTop Retrieved Chunks:")
for i, snippet in enumerate(result["retrieved_texts"], 1):
    print(f"{i}. ({result['scores'][i-1]:.3f}) {snippet}")

query = "What is stableLM?"
result = retrieval_qa(query)

print("Answer:\n", result["answer"])
print("\nTop Retrieved Chunks:")
for i, snippet in enumerate(result["retrieved_texts"], 1):
    print(f"{i}. ({result['scores'][i-1]:.3f}) {snippet}")